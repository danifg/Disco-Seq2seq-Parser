set -o errexit 
set -o pipefail
# setup environment
. set_environment.sh
# Argument handling
config=$1
checkpoint=$2
results_path=$3
HELP="\ntest.sh <config> <model_checkpoint> [<results_path>]\n"
[ -z "$config" ] && echo -e $HELP && exit 1
[ -z "$checkpoint" ] && echo -e $HELP && exit 1
[ -z "$results_path" ] && results_path=""
set -o nounset 

# Load config
. "$config"

# Default path
if [ "$results_path" == "" ];then
    # fix for ensembles
    single_checkpoint=$(echo $checkpoint | sed 's@\.pt:.*@@')
    results_path=$(dirname $single_checkpoint)/$TEST_TAG/test
fi
mkdir -p $(dirname $results_path)


echo $results_path

# to profile decoder
# decorate target function with @profile
# test_command="kernprof -o generate.lprof -l fairseq/generate.py"
# python -m line_profiler generate.py.lprof
test_command=fairseq-generate

# Decode to get predicted action sequence
if [ ! -f "${results_path}.actions" ];then
    echo "$test_command $FAIRSEQ_GENERATE_ARGS --path $checkpoint --results-path ${results_path}"
    $test_command $FAIRSEQ_GENERATE_ARGS \
        --path $checkpoint \
        --results-path ${results_path} 
fi


# FOR EACH TASK EVALUATE FOR EACH OF THE SUB TASKS INVOLVED e.g. AMR+NER
for single_task  in $(python -c "print(' '.join('$TASK_TAG'.split('+')))");do

      

    if [ "$single_task" == "dep-parsing" ];then
    
        # dep-parsing (UAS/LAS)
        python scripts/dep_parsing_score.py \
            --in-tokens $ORACLE_FOLDER/test.en \
            --in-actions ${results_path}.actions \
            --in-gold-actions $ORACLE_FOLDER/test.actions \
            > ${results_path}.las
        cat ${results_path}.las

    else
 
        echo "Unsupported task $single_task"
        exit 1

    fi
 
done
